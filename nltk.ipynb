{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83a7c519-c054-4a93-a4d1-d3283a5fb92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.55.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388bf1a5-4682-471a-b8f7-adb7d0d79008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing nlp is amazing it helps computers understand human language\n"
     ]
    }
   ],
   "source": [
    "#Remove Punctuation\n",
    "import re\n",
    "text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96f9177d-d481-435b-81e5-c6e7fb31c3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'is', 'amazing', 'it', 'helps', 'computers', 'understand', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize (text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87bdb025-9d20-420b-abe1-651ebd60b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'amazing', 'helps', 'computers', 'understand', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "#Remove Stopwords\n",
    "#List of English stop words: is, the, and, in, it, etc.\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in\n",
    "stop_words]\n",
    "print (filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19ad3569-06b5-43ed-bdc8-82061b74eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'nlp', 'amaz', 'help', 'comput', 'understand', 'human', 'languag']\n"
     ]
    }
   ],
   "source": [
    "#Stemming & Lemmatization\n",
    "#Stemming is a rule-based process of chopping off word endings (suffixes) to get to the root form.\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e779bf8-3fcc-4fda-83f1-a0a44c546920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'amazing', 'help', 'computer', 'understand', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "#Stemming & Lemmatization\n",
    "#Lemmatization is a smarter, dictionary-based approach. It converts words to their base or dictionary form, called a lemma.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize (word) for word in\n",
    "filtered_tokens]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561781f-5fdf-4660-872c-10716d09bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess text using NLTK before feeding to NLP.\n",
    "#step1: insatll library\n",
    "#step2: Remove Punctuation\n",
    "#step3:Tokenization\n",
    "#step4:Remove Stopwords\n",
    "#step 5:Stemming & Lemmatization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
